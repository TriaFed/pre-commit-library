#!/usr/bin/env python3
"""
GenAI-specific security validation for code generated by AI tools like GitHub Copilot.
Checks for common security issues and anti-patterns in AI-generated code.
"""

import re
import sys
import argparse
import ast
import json
from typing import List, Tuple, Dict, Set, Any
from pathlib import Path

# Common GenAI security anti-patterns
GENAI_SECURITY_PATTERNS = {
    'insecure_random': {
        'patterns': [
            r'Math\.random\(\)',
            r'random\.random\(\)',
            r'new Random\(\)',
            r'rand\(\)',
        ],
        'description': 'Use cryptographically secure random generators for security-sensitive operations',
        'severity': 'medium'
    },
    'sql_injection': {
        'patterns': [
            r'SELECT.*\+.*',
            r'INSERT.*\+.*',
            r'UPDATE.*\+.*',
            r'DELETE.*\+.*',
            r'query\s*\+\s*',
            r'executeQuery\([^)]*\+[^)]*\)',
        ],
        'description': 'Potential SQL injection vulnerability - use parameterized queries',
        'severity': 'high'
    },
    'command_injection': {
        'patterns': [
            r'exec\s*\([^)]*\+[^)]*\)',
            r'system\s*\([^)]*\+[^)]*\)',
            r'subprocess\..*shell=True.*',
            r'os\.system\([^)]*\+[^)]*\)',
            r'Runtime\.getRuntime\(\)\.exec\([^)]*\+[^)]*\)',
        ],
        'description': 'Potential command injection - avoid dynamic command construction',
        'severity': 'high'
    },
    'path_traversal': {
        'patterns': [
            r'\.\./',
            r'\.\.\\',
            r'path.*\+.*',
            r'File\([^)]*\+[^)]*\)',
        ],
        'description': 'Potential path traversal vulnerability - validate file paths',
        'severity': 'high'
    },
    'weak_crypto': {
        'patterns': [
            r'MD5',
            r'SHA1\b',
            r'DES\b',
            r'RC4',
            r'\.md5\(',
            r'\.sha1\(',
            r'createHash\(["\']md5["\']',
            r'createHash\(["\']sha1["\']',
        ],
        'description': 'Weak cryptographic algorithm - use SHA-256 or stronger',
        'severity': 'medium'
    },
    'hardcoded_crypto': {
        'patterns': [
            r'Cipher.*"[A-Za-z0-9+/]{16,}"',
            r'encrypt.*"[A-Za-z0-9+/]{16,}"',
            r'decrypt.*"[A-Za-z0-9+/]{16,}"',
            r'createCipher.*"[^"]*".*"[A-Za-z0-9+/]{8,}"',
        ],
        'description': 'Hardcoded cryptographic keys - use secure key management',
        'severity': 'high'
    },
    'eval_usage': {
        'patterns': [
            r'\beval\s*\(',
            r'Function\s*\(',
            r'setTimeout\s*\([^,)]*["\'][^"\']*["\']',
            r'setInterval\s*\([^,)]*["\'][^"\']*["\']',
            r'Script\.eval',
        ],
        'description': 'Dynamic code execution - avoid eval() and similar functions',
        'severity': 'high'
    },
    'xxe_vulnerability': {
        'patterns': [
            r'DocumentBuilderFactory.*newInstance',
            r'SAXParserFactory.*newInstance',
            r'XMLInputFactory.*newInstance',
            r'TransformerFactory.*newInstance',
        ],
        'description': 'Potential XXE vulnerability - disable external entity processing',
        'severity': 'medium'
    },
    'insecure_deserialization': {
        'patterns': [
            r'pickle\.loads\(',
            r'yaml\.load\(',
            r'ObjectInputStream',
            r'readObject\(\)',
            r'JSON\.parse\([^)]*untrusted',
        ],
        'description': 'Insecure deserialization - validate input and use safe parsers',
        'severity': 'high'
    },
    'information_disclosure': {
        'patterns': [
            r'printStackTrace\(\)',
            r'console\.error\([^)]*error[^)]*\)',
            r'print\([^)]*exception[^)]*\)',
            r'echo.*\$.*error',
            r'response\.write\([^)]*error[^)]*\)',
        ],
        'description': 'Potential information disclosure - avoid exposing stack traces',
        'severity': 'low'
    },
    'csrf_missing': {
        'patterns': [
            r'@RequestMapping.*method.*POST.*',
            r'@PostMapping.*',
            r'app\.post\(',
            r'\.post\(\s*["\'][^"\']*["\']',
        ],
        'description': 'POST endpoint without CSRF protection - verify CSRF tokens',
        'severity': 'medium'
    }
}

# GenAI comment patterns that might indicate security issues
SUSPICIOUS_COMMENT_PATTERNS = [
    r'TODO.*security',
    r'FIXME.*security',
    r'hack.*',
    r'workaround.*',
    r'temporary.*fix',
    r'bypass.*',
    r'ignore.*security',
    r'disable.*security',
    r'unsafe.*',
    r'insecure.*',
]

# File extensions to analyze
SUPPORTED_EXTENSIONS = {'.py', '.js', '.ts', '.java', '.php', '.cs', '.cpp', '.c', '.go', '.rb', '.scala'}


def analyze_python_ast(file_path: str, content: str) -> List[Tuple[int, str, str]]:
    """Analyze Python AST for security issues."""
    issues = []
    
    try:
        tree = ast.parse(content)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name):
                    # Check for dangerous function calls
                    if node.func.id == 'eval':
                        issues.append((
                            node.lineno,
                            'eval() usage detected',
                            'Dynamic code execution vulnerability'
                        ))
                    elif node.func.id == 'exec':
                        issues.append((
                            node.lineno,
                            'exec() usage detected', 
                            'Dynamic code execution vulnerability'
                        ))
                
                elif isinstance(node.func, ast.Attribute):
                    # Check for subprocess with shell=True
                    if (hasattr(node.func, 'attr') and 
                        node.func.attr in ['call', 'run', 'Popen']):
                        for keyword in node.keywords:
                            if (keyword.arg == 'shell' and 
                                isinstance(keyword.value, ast.Constant) and 
                                keyword.value.value is True):
                                issues.append((
                                    node.lineno,
                                    'subprocess with shell=True',
                                    'Command injection vulnerability'
                                ))
    
    except SyntaxError:
        pass  # Skip files with syntax errors
    
    return issues


def check_genai_patterns(file_path: str) -> List[Tuple[int, str, str, str]]:
    """Check for GenAI-specific security patterns."""
    issues = []
    
    # Skip if not a supported file type
    if not any(file_path.endswith(ext) for ext in SUPPORTED_EXTENSIONS):
        return issues
    
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
            lines = content.split('\n')
            
            # Pattern-based analysis
            for line_num, line in enumerate(lines, 1):
                if not line.strip():
                    continue
                
                # Check security patterns
                for pattern_name, pattern_info in GENAI_SECURITY_PATTERNS.items():
                    for pattern in pattern_info['patterns']:
                        if re.search(pattern, line, re.IGNORECASE):
                            issues.append((
                                line_num,
                                line.strip(),
                                pattern_name,
                                pattern_info['description']
                            ))
                
                # Check suspicious comments
                for comment_pattern in SUSPICIOUS_COMMENT_PATTERNS:
                    if re.search(comment_pattern, line, re.IGNORECASE):
                        issues.append((
                            line_num,
                            line.strip(),
                            'suspicious_comment',
                            'Suspicious comment indicating potential security issue'
                        ))
            
            # AST-based analysis for Python files
            if file_path.endswith('.py'):
                ast_issues = analyze_python_ast(file_path, content)
                for line_num, issue_type, description in ast_issues:
                    issues.append((line_num, '', issue_type, description))
    
    except Exception as e:
        print(f"Error analyzing {file_path}: {e}", file=sys.stderr)
    
    return issues


def get_severity_emoji(pattern_name: str) -> str:
    """Get emoji based on severity."""
    if pattern_name in GENAI_SECURITY_PATTERNS:
        severity = GENAI_SECURITY_PATTERNS[pattern_name]['severity']
        return {
            'high': 'ğŸš¨',
            'medium': 'âš ï¸',
            'low': 'ğŸ’¡'
        }.get(severity, 'âš ï¸')
    return 'âš ï¸'


def main():
    parser = argparse.ArgumentParser(description='GenAI-specific security validation')
    parser.add_argument('files', nargs='*', help='Files to check')
    parser.add_argument('--severity', choices=['low', 'medium', 'high'],
                        help='Minimum severity level to report')
    parser.add_argument('--json', action='store_true',
                        help='Output results in JSON format')
    args = parser.parse_args()
    
    exit_code = 0
    total_issues = 0
    all_results = {}
    
    severity_levels = {'low': 1, 'medium': 2, 'high': 3}
    min_severity = severity_levels.get(args.severity, 1) if args.severity else 1
    
    for file_path in args.files:
        issues = check_genai_patterns(file_path)
        
        # Filter by severity if specified
        if args.severity:
            filtered_issues = []
            for line_num, line_content, pattern_name, description in issues:
                if pattern_name in GENAI_SECURITY_PATTERNS:
                    pattern_severity = GENAI_SECURITY_PATTERNS[pattern_name]['severity']
                    if severity_levels.get(pattern_severity, 1) >= min_severity:
                        filtered_issues.append((line_num, line_content, pattern_name, description))
                else:
                    filtered_issues.append((line_num, line_content, pattern_name, description))
            issues = filtered_issues
        
        if issues:
            if args.json:
                all_results[file_path] = [
                    {
                        'line': line_num,
                        'content': line_content,
                        'pattern': pattern_name,
                        'description': description
                    }
                    for line_num, line_content, pattern_name, description in issues
                ]
            else:
                print(f"\nğŸ¤– GenAI security issues found in {file_path}:")
                for line_num, line_content, pattern_name, description in issues:
                    emoji = get_severity_emoji(pattern_name)
                    print(f"  {emoji} Line {line_num}: {pattern_name.replace('_', ' ').title()}")
                    print(f"    Description: {description}")
                    if line_content:
                        print(f"    Code: {line_content}")
            
            total_issues += len(issues)
            exit_code = 1
    
    if args.json:
        print(json.dumps(all_results, indent=2))
    elif total_issues > 0:
        print(f"\nâŒ Found {total_issues} potential GenAI security issue(s)")
        print("ğŸ’¡ Review AI-generated code carefully for security vulnerabilities")
        print("ğŸ’¡ These patterns are common in AI-generated code that needs security review")
    else:
        print("âœ… No GenAI security issues detected")
    
    return exit_code


if __name__ == '__main__':
    sys.exit(main())
